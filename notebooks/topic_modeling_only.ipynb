{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bertopic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05aeb861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/compss211/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import bertopic\n",
    "from bertopic import BERTopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee7270e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3308617, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('SPOTIFY_REVIEWS_CLEANED.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f48726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 3308617\n",
      "After the most recent 5 year: 1669701\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "df['review_timestamp'] = pd.to_datetime(df['review_timestamp'])\n",
    "df2 = df[(df['review_timestamp'] >= '2019-11-15') & (df['review_timestamp'] <= '2023-11-15')]\n",
    "print(\"Original:\", len(df))\n",
    "print(\"After the most recent 5 year:\", len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ab433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_type\n",
      "Very short    614051\n",
      "Long          587948\n",
      "Short         269962\n",
      "Medium        197652\n",
      "Name: count, dtype: int64\n",
      "length_type\n",
      "Long      587948\n",
      "Medium    197652\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_final = df2[~df2['length_type'].isin(['Short', 'Very short'])].reset_index(drop=True)\n",
    "print(df2['length_type'].value_counts())\n",
    "print(df_final['length_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31afd4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_id</th>\n",
       "      <th>pseudo_author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_likes</th>\n",
       "      <th>author_app_version</th>\n",
       "      <th>review_timestamp</th>\n",
       "      <th>raw_word_count</th>\n",
       "      <th>length_type</th>\n",
       "      <th>length_type2</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1663991</td>\n",
       "      <td>979989f1-78a2-4576-a783-c763ae7a9ffa</td>\n",
       "      <td>157768270865747512306</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>I love the fact that I can listen to nearly an...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5.31.676</td>\n",
       "      <td>2019-11-15 00:02:50</td>\n",
       "      <td>68.0</td>\n",
       "      <td>Long</td>\n",
       "      <td>Long</td>\n",
       "      <td>['i', 'love', 'the', 'fact', 'that', 'i', 'can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1663997</td>\n",
       "      <td>bd98f73f-1bb1-42f8-ad7a-d12f4c3662e9</td>\n",
       "      <td>280812221700598190021</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Randomly stops playing.... I will be listening...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8.5.31.676</td>\n",
       "      <td>2019-11-15 00:18:54</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Long</td>\n",
       "      <td>Long</td>\n",
       "      <td>['randomly', 'stops', 'playing', '.', '.', '.'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1663998</td>\n",
       "      <td>7167a53f-b1c0-4eae-9f33-26295c73de76</td>\n",
       "      <td>765391996510868237903</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>So far this has been a better experience than ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5.31.676</td>\n",
       "      <td>2019-11-15 00:19:11</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Long</td>\n",
       "      <td>Long</td>\n",
       "      <td>['so', 'far', 'this', 'has', 'been', 'a', 'bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1664000</td>\n",
       "      <td>9fb431b7-7f2b-494c-8f0d-2805dec40b70</td>\n",
       "      <td>307958352133874143584</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>Great app for looking up and listening to ur f...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-15 00:20:11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Long</td>\n",
       "      <td>Long</td>\n",
       "      <td>['great', 'app', 'for', 'looking', 'up', 'and'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1664001</td>\n",
       "      <td>e3872654-d562-422e-96c1-d2480db446f4</td>\n",
       "      <td>180538848993703574960</td>\n",
       "      <td>A Google user</td>\n",
       "      <td>I love this app so .uch cause zi get to listen...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5.29.828</td>\n",
       "      <td>2019-11-15 00:20:20</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Long</td>\n",
       "      <td>Long</td>\n",
       "      <td>['i', 'love', 'this', 'app', 'so', '.', 'uc', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                             review_id       pseudo_author_id  \\\n",
       "0     1663991  979989f1-78a2-4576-a783-c763ae7a9ffa  157768270865747512306   \n",
       "1     1663997  bd98f73f-1bb1-42f8-ad7a-d12f4c3662e9  280812221700598190021   \n",
       "2     1663998  7167a53f-b1c0-4eae-9f33-26295c73de76  765391996510868237903   \n",
       "3     1664000  9fb431b7-7f2b-494c-8f0d-2805dec40b70  307958352133874143584   \n",
       "4     1664001  e3872654-d562-422e-96c1-d2480db446f4  180538848993703574960   \n",
       "\n",
       "     author_name                                        review_text  \\\n",
       "0  A Google user  I love the fact that I can listen to nearly an...   \n",
       "1  A Google user  Randomly stops playing.... I will be listening...   \n",
       "2  A Google user  So far this has been a better experience than ...   \n",
       "3  A Google user  Great app for looking up and listening to ur f...   \n",
       "4  A Google user  I love this app so .uch cause zi get to listen...   \n",
       "\n",
       "   review_rating  review_likes author_app_version    review_timestamp  \\\n",
       "0              4             1         8.5.31.676 2019-11-15 00:02:50   \n",
       "1              2             2         8.5.31.676 2019-11-15 00:18:54   \n",
       "2              5             0         8.5.31.676 2019-11-15 00:19:11   \n",
       "3              5             0                NaN 2019-11-15 00:20:11   \n",
       "4              5             0         8.5.29.828 2019-11-15 00:20:20   \n",
       "\n",
       "   raw_word_count length_type length_type2  \\\n",
       "0            68.0        Long         Long   \n",
       "1            64.0        Long         Long   \n",
       "2            22.0        Long         Long   \n",
       "3            11.0        Long         Long   \n",
       "4            18.0        Long         Long   \n",
       "\n",
       "                                              tokens  \n",
       "0  ['i', 'love', 'the', 'fact', 'that', 'i', 'can...  \n",
       "1  ['randomly', 'stops', 'playing', '.', '.', '.'...  \n",
       "2  ['so', 'far', 'this', 'has', 'been', 'a', 'bet...  \n",
       "3  ['great', 'app', 'for', 'looking', 'up', 'and'...  \n",
       "4  ['i', 'love', 'this', 'app', 'so', '.', 'uc', ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = df_final[~df_final['review_text'].isin(['[deleted]', '[removed]'])].reset_index(drop=True)\n",
    "df_final.shape\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a9c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def light_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df_final['review_text_clean'] = df_final['review_text'].apply(light_clean)\n",
    "docs = df_final['review_text_clean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8214268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (0.17.3)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (5.1.2)\n",
      "Requirement already satisfied: umap-learn in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (0.5.9.post2)\n",
      "Requirement already satisfied: hdbscan in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (0.8.40)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from bertopic) (2.3.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from bertopic) (2.3.3)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from bertopic) (6.4.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from bertopic) (4.67.1)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from bertopic) (0.45.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from sentence-transformers) (2.9.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.5.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from umap-learn) (0.62.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from hdbscan) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from plotly>=4.7.0->bertopic) (2.10.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/compss211/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install scikit-learn\n",
    "#%pip install PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from bertopic import BERTopic\n",
    "\n",
    "%pip install bertopic sentence-transformers umap-learn hdbscan scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f48fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 12:06:03,798 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 24553/24553 [50:52<00:00,  8.04it/s]  \n",
      "2025-11-10 12:57:25,346 - BERTopic - Embedding - Completed ✓\n",
      "2025-11-10 12:57:25,495 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', min_dist=0.0, n_components=5, n_jobs=1, random_state=42, verbose=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 10 12:57:29 2025 Construct fuzzy simplicial set\n",
      "Mon Nov 10 12:57:30 2025 Finding Nearest Neighbors\n",
      "Mon Nov 10 12:57:30 2025 Building RP forest with 49 trees\n",
      "Mon Nov 10 13:00:32 2025 NN descent for 20 iterations\n",
      "\t 1  /  20\n",
      "\t 2  /  20\n",
      "\t 3  /  20\n",
      "\t 4  /  20\n",
      "\t 5  /  20\n",
      "\t 6  /  20\n",
      "\tStopping threshold met -- exiting after 6 iterations\n",
      "Mon Nov 10 13:03:38 2025 Finished Nearest Neighbor Search\n",
      "Mon Nov 10 13:03:55 2025 Construct embedding\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#simplifier model(chatgpt)\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import umap, hdbscan, torch\n",
    "\n",
    "# 1) Pick a *small* but strong SBERT (fast)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "emb_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# 2) Fast vectorizer (removes punctuation via token_pattern; English stopwords)\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,            # raise for speed on big corpora\n",
    "    max_df=0.95,\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z]+\\b\"  # words ≥2 letters; skips punctuation/digits\n",
    ")\n",
    "\n",
    "# 3) Aggressive dimensionality reduction (fewer components = faster)\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,      # ↓ speeds clustering a lot; try 5–10\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4) Faster density clustering (larger min_cluster_size = fewer, faster clusters)\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=50,     # tune: larger = faster, coarser topics\n",
    "    metric=\"euclidean\",      # BERTopic default after UMAP\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=False    # disable soft probs for speed\n",
    ")\n",
    "\n",
    "# 5) Build the fast BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=emb_model,\n",
    "    vectorizer_model=vectorizer,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    language=\"english\",\n",
    "    low_memory=True,\n",
    "    calculate_probabilities=False,  # big speed win\n",
    "    nr_topics=\"auto\",               # or an int to force a target # of topics\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ---- Your docs list here (list[str]) ----\n",
    "# Example:\n",
    "# docs = df['review_text'].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)  # probs=None since disabled\n",
    "\n",
    "# Inspect\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head())            # topic sizes\n",
    "print(topic_model.get_topic(0)[:10])  # top terms for topic 0\n",
    "\n",
    "# Representative docs per topic\n",
    "rep = topic_model.get_representative_docs()\n",
    "# rep is {topic_id: [doc1, doc2, ...]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a small + fast BERT embedding model\n",
    "emb_model2 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Create BERTopic with defaults (simple mode)\n",
    "topic_model2 = BERTopic(embedding_model=emb_model)\n",
    "\n",
    "\n",
    "# 4. Fit and get topic assignments\n",
    "topics, probs = topic_model2.fit_transform(docs)\n",
    "\n",
    "# 5. View top topics\n",
    "topic_info2 = topic_model2.get_topic_info()\n",
    "print(topic_info2)\n",
    "\n",
    "# 6. View top words for a topic (example: topic 0)\n",
    "print(topic_model2.get_topic(0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compss211",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
