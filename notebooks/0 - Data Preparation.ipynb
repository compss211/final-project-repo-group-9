{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from csv file in data folder\n",
    "df = pd.read_csv(\"../data/SPOTIFY_REVIEWS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was filtered to include only the most recent five years of reviews, from November 15, 2019, to November 15, 2023, ensuring the analysis captures the current trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 3377423\n",
      "After the most recent 5 year: 1711607\n"
     ]
    }
   ],
   "source": [
    "# Filter reviews from the most recent 5 years\n",
    "df['review_timestamp'] = pd.to_datetime(df['review_timestamp'])\n",
    "df2 = df[(df['review_timestamp'] >= '2019-11-15') & (df['review_timestamp'] <= '2023-11-15')]\n",
    "\n",
    "# Display the number of reviews before and after filtering\n",
    "print(\"Original:\", len(df))\n",
    "print(\"After the most recent 5 year:\", len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous data exploration analysis revealed that some reviews contained unusually long words. To enhance data quality, we implemented a filter to exclude reviews with words exceeding 15 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering long words: 1669701\n"
     ]
    }
   ],
   "source": [
    "# Function to check for very long words\n",
    "def has_very_long_word(text, max_len=15):\n",
    "    # make sure it's string\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    for w in text.split():\n",
    "        if len(w) >= max_len:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Filter out reviews with very long words\n",
    "mask_long = df2[\"review_text\"].apply(has_very_long_word)\n",
    "df_clean = df2[~mask_long].copy()\n",
    "\n",
    "# Display the number of reviews after filtering\n",
    "print(\"After filtering long words:\", len(df_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Length Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables `raw_word_count`, `length_type`, and `length_type2` were created to categorize review length for further analysis. The four-category `length_type` allows for granular examination, which is useful for isolating reviews of a suitable complexity for specific tasks; for instance, very short reviews may lack the textual depth required for reliable topic modeling. The binary `length_type2` enables a direct comparison between short and long reviews, facilitating the development and evaluation of separate predictive models for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate raw word count and create length type categories\n",
    "df_clean['raw_word_count'] = df_clean['review_text'].str.split().str.len()\n",
    "df_clean['length_type'] = pd.cut(df_clean['raw_word_count'], \n",
    "                                bins=[0, 3, 6, 10, float('inf')], \n",
    "                                labels=['Very short', 'Short', 'Medium', 'Long'])\n",
    "df_clean['length_type2'] = pd.cut(df_clean['raw_word_count'], \n",
    "                                bins=[0, 6, float('inf')], \n",
    "                                labels=['Short','Long'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT tokenizer was employed to segment review text into subword units (e.g., \"playing\" -> \"play\", \"##ing\"). This approach effectively handles out-of-vocabulary words that are common in informal user reviews, such as slang, misspellings, and product-specific terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BERT tokenizer\n",
    "df_clean[\"review_text\"] = df_clean[\"review_text\"].astype(str)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "for i in range(5): \n",
    "    text = df_clean.loc[i, \"review_text\"]\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"\\n--- Review {i+1} ---\")\n",
    "    print(\"Original:\", text)\n",
    "    print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the review text using BERT tokenizer\n",
    "df_clean[\"tokens\"] = df_clean[\"review_text\"].apply(lambda x: tokenizer.tokenize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1663991    [i, love, the, fact, that, i, can, listen, to,...\n",
       "1663992                                       [awesome, app]\n",
       "1663993                                      [really, [UNK]]\n",
       "1663994                                           [love, it]\n",
       "1663995                                   [liked, mast, ##i]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few tokenized reviews\n",
    "df_clean[\"tokens\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and tokenized DataFrame to a CSV file\n",
    "df_clean.to_csv(\"../data/SPOTIFY_REVIEWS_tokens.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "hex_info": {
   "author": "Leyi Song",
   "exported_date": "Sun Nov 09 2025 19:32:35 GMT+0000 (Coordinated Universal Time)",
   "project_id": "019a5ff1-5836-7001-860a-81e94d77fe63",
   "version": "draft"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
